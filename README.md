# The Full code will be updated after the review is done for this Paper
– Developed lightweight, energy-efficient diagnostic framework using teacher-student knowledge distillation with 30%
model size reduction
– Trained DSNet student model via EfficientNetB0 teacher achieving 97.12% accuracy with 70% lower carbon foot-
print compared to baseline models
– Compared traditional Knowledge Distillation and multi-teacher Knowledge Distillation methods for performance,
scalability, and environmental impact across 25,000 CT scan augmented images
– Incorporated Grad-CAM and LIME for clinical-grade explainability providing pixel-level attention maps for trans-
parent decision-making
– Tools: TensorFlow 2.8, Keras, Vision Transformers, LIME, Grad-CAM, PyTorch
